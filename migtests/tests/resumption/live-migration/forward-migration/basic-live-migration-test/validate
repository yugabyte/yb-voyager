We have created the custom_hash_code function
that is

CREATE OR REPLACE FUNCTION custom_hash_code(pk_value BIGINT)
RETURNS INTEGER AS $$
SELECT
    -- Cast the largest multiplier (16777216) to BIGINT to force 8-byte arithmetic, 
    -- preventing the intermediate overflow error.
    (
        get_byte(decode(md5(pk_value::TEXT), 'hex'), 0) * 16777216::BIGINT +
        get_byte(decode(md5(pk_value::TEXT), 'hex'), 1) * 65536 +
        get_byte(decode(md5(pk_value::TEXT), 'hex'), 2) * 256 +
        get_byte(decode(md5(pk_value::TEXT), 'hex'), 3)
    ) % 65536;
$$ LANGUAGE SQL IMMUTABLE;

Our next step was to create the hash table. This will be separate for each table. Or we can have a single hash table for all tables. Whatever is the best code design. But since we have source_table as a column, we can use it to filter.

CREATE TABLE public.xcluster_validate (
    source_schema TEXT,
    source_table TEXT,
    low_range BIGINT,
    high_range BIGINT,
    segment_hash TEXT,
    -- Add a primary key for efficient comparison and potential indexing
    PRIMARY KEY (source_table, low_range, high_range) 
);

Then we inserted the hash values into the table. This is an example of one table and one segment. We will have to do this dynamically for all tables and segments.

INSERT INTO public.xcluster_validate 
(source_schema, source_table, low_range, high_range, segment_hash)
SELECT
    C.source_schema,
    C.source_table,
    C.low_range,
    C.high_range,
    H.final_hash
FROM (
    -- CONSTANTS TABLE (C): Defines the fixed strings and range boundaries
    SELECT
        'public' AS source_schema,
        'num_types' AS source_table,
        0::BIGINT AS low_range,
        4096::BIGINT AS high_range
) AS C
CROSS JOIN (
    -- HASH CALCULATION TABLE (H): Calculates the aggregated hash
    SELECT
        md5(string_agg(row_hash, '' ORDER BY id)) AS final_hash
    FROM (
        -- Per-Row Hash Subquery (R): Calculates the stable hash for each row
        SELECT
            u.id,
            md5(
                CAST(u.id AS TEXT) || '|' ||
                COALESCE(CAST(u.v1 AS TEXT), 'NULL') || '|' ||
                COALESCE(CAST(u.v2 AS TEXT), 'NULL') || '|' ||
                COALESCE(CAST(u.v3 AS TEXT), 'NULL') || '|' ||
                COALESCE(CAST(u.v4 AS TEXT), 'NULL') || '|' ||
                COALESCE(CAST(u.v6 AS TEXT), 'NULL')
            ) AS row_hash
        FROM num_types u
        WHERE custom_hash_code(u.id) >= 0 AND custom_hash_code(u.id) < 4096
    ) AS R
) AS H;

We will compare the 16 rows of each table on source and target. If it matches, well and good.

If it does not,

We need to check the segment wherein the mismatch is happening for a table.

SELECT
    u.id,
    -- Per-row hash logic (stable for comparison)
    md5(
        CAST(u.id AS TEXT) || '|' ||
        COALESCE(CAST(u.v1 AS TEXT), 'NULL') || '|' ||
        COALESCE(CAST(u.v2 AS TEXT), 'NULL') || '|' ||
        COALESCE(CAST(u.v3 AS TEXT), 'NULL') || '|' ||
        COALESCE(CAST(u.v4 AS TEXT), 'NULL') || '|' ||
        COALESCE(CAST(u.v6 AS TEXT), 'NULL')
    ) AS row_hash
FROM num_types u
-- FIX: Replaced placeholders with actual numbers
WHERE custom_hash_code(u.id) >= 4096 AND custom_hash_code(u.id) < 8192
ORDER BY u.id;

This will give us all the rows in that segment matching or not.

Then we filter out the non-matching rows based on the id.


