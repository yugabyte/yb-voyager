#!/usr/bin/env bash

set -e
set -x

export YB_VOYAGER_SEND_DIAGNOSTICS=false
export TEST_NAME="import-file"

export REPO_ROOT="${PWD}"
export SCRIPTS="${REPO_ROOT}/migtests/scripts"
export TESTS_DIR="${REPO_ROOT}/migtests/tests"
export TEST_DIR="${TESTS_DIR}/${TEST_NAME}"
export EXPORT_DIR=${EXPORT_DIR:-"${TEST_DIR}/export-dir"}

export S3_BUCKET="s3://voyager-automation-data"
export AWS_DEFAULT_REGION="us-west-2"

export PYTHONPATH="${REPO_ROOT}/migtests/lib"

source ${SCRIPTS}/yugabytedb/env.sh
source ${SCRIPTS}/functions.sh

export TARGET_DB_NAME="testdb"

main() {
	rm -rf ${EXPORT_DIR}
	mkdir -p ${EXPORT_DIR}

	pushd ${TEST_DIR}

	step "Create target database."
	run_ysql yugabyte "DROP DATABASE IF EXISTS ${TARGET_DB_NAME};"
	run_ysql yugabyte "CREATE DATABASE ${TARGET_DB_NAME}"

	step "Unzip the data file."
	[ -f OneMRows.text ] || gunzip -c OneMRows.text.gz > OneMRows.text
	[ -f accounts_data.csv ] || gunzip -c accounts.zip > accounts_data.csv

	step "Create target table."
	ysql_import_file ${TARGET_DB_NAME} schema.sql

	export TARGET_DB_SCHEMA='non_public'

# Kept --start-clean here to test the command for import data file. Can add proper validations for it once --truncate-tables is introduced here

	step "Import data file: SMSA.txt -> smsa in a non-public schema"
	import_data_file --data-dir ${TEST_DIR} --format text --delimiter '\t' \
			--file-table-map "SMSA.txt:smsa" --start-clean true
	
	#clean up the export dir as we will have public schema from this test which should be on fresh export-dir 
	rm -rf ${EXPORT_DIR}
	mkdir -p ${EXPORT_DIR}
	export TARGET_DB_SCHEMA='public'

	#Default BATCH_SIZE_BYTES
	step "Import data file: OneMRows.text -> one_m_rows"
	import_data_file --data-dir ${TEST_DIR} --format text --delimiter '|' \
		--file-table-map "OneMRows.text:one_m_rows"

	
	export MAX_BATCH_SIZE_BYTES=345643 #~300KB 

	export CSV_READER_MAX_BUFFER_SIZE_BYTES=300
	step "Import data file: FY2021_Survey.csv -> survey"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter ',' \
		--file-table-map "FY2021_Survey.csv:survey" --has-header=true 

	# Test for multiple table files import
	step "Import data file: FY2021_Survey.csv -> survey2 and FY2021_Survey.csv -> survey3"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter ',' \
		--file-table-map "FY2021_Survey.csv:survey2,FY2021_Survey.csv:survey3" \
		--has-header=true --batch-size 1000 

	# Test for --has-header with --batch-size for a table having bigint and varchar columns
	step "Import data file: accounts_data.csv -> accounts"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '\t' \
			--file-table-map "accounts_data.csv:accounts" --has-header=true --batch-size 10000 --null-string "\N"

	#Test for Large rows and non-default batch size bytes (i.e. 17MB for row sizes of 15MB,11MB,5KB,12MB,9.9MB)
	step "Import data file: large_rows_test.txt -> large_rows_test" 
	export MAX_BATCH_SIZE_BYTES=17000000
	import_data_file --data-dir ${TEST_DIR} --format text --delimiter '\t' \
			--file-table-map "large_rows_test.txt:large_rows_test"
	
	# import data state metainfo directory for large_rows_test table
	IMPORT_DATA_STATE_DIR_LARGE_ROWS="$TEST_DIR/export-dir/metainfo/import_data_state/import_file/table::public.\"large_rows_test\"/"

	# Expected batch file names without the symlink for large_rows_test.txt file
	expected_batch_files_large_rows=(
		"batch::0.5.1.9900003.D"
		"batch::1.1.1.15000003.D"
		"batch::2.3.2.11005006.D"
		"batch::3.4.1.12131223.D"
	)

	validate_import_data_state_batch_files $IMPORT_DATA_STATE_DIR_LARGE_ROWS ${expected_batch_files_large_rows[@]}
	
	export MAX_BATCH_SIZE_BYTES=3032 #these are smaller files so its better to have smaller here 

	# Next 4 tests are right now supported with a special csv format i.e. without new line
	# for complete support of csv with newline, track this issue - https://github.com/yugabyte/yb-voyager/issues/748
	#Test for fileOpts Flags having quote_char as single quote
	step "Import data file: t1_quote_char.csv -> t1_quote_char"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '|' \
			--file-table-map "t1_quote_char.csv:t1_quote_char" --quote-char="'"

	#Test for fileOpts Flags having quote_char as single quote and escape_char as single quote 
	#file-opts is deprecated just keeping for backward compatibility test
	step "Import data file: t1_quote_escape_char1.csv -> t1_quote_escape_char1"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '|' \
			--file-table-map "t1_quote_escape_char1.csv:t1_quote_escape_char1" --file-opts "quote_char=',escape_char='"

	#Test for fileOpts Flags having quote_char as single quote and escape_char as backslash
	step "Import data file: t1_quote_escape_char2.csv -> t1_quote_escape_char2"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '|' \
			--file-table-map "t1_quote_escape_char2.csv:t1_quote_escape_char2" --quote-char="'" --escape-char="\\"
	

	#Test in case delimiter is same as escape character
	step "Import data file: t1_delimiter_escape_same.csv -> t1_delimiter_escape_same"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '|' \
			--file-table-map "t1_delimiter_escape_same.csv:t1_delimiter_escape_same" --quote-char="'" --escape-char="|"

	# Test for csv file containing actual newline in it
	step "Import data file: t1_newline.csv -> t1_newline"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter ',' \
			--file-table-map "t1_newline.csv:t1_newline"

	# Test for text file for default delimiter
	step "Import data file: test_default_delimiter.csv -> test_default_delimiter"
	import_data_file --data-dir ${TEST_DIR} --format text  \
			--file-table-map "test_default_delimiter.txt:test_default_delimiter"

	step "Import data file: test_default_delimiter_csv.csv -> test_default_delimiter_csv"
	import_data_file --data-dir ${TEST_DIR} --format csv  \
			--file-table-map "test_default_delimiter_csv.csv:test_default_delimiter_csv"

	# Test for csv file with default escape and quote character
	step "Import data file: t1_quote_escape_dq.csv -> t1_quote_escape_dq"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '|' \
			--file-table-map "t1_quote_escape_dq.csv:t1_quote_escape_dq"

	# Test for csv file with backslash as escape and default quote character having multiple double quote strings in varchar field
	step "Import data file: t1_escape_backslash.csv -> t1_escape_backslash"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter ',' \
			--file-table-map "t1_escape_backslash.csv:t1_escape_backslash" --escape-char="\\"

	# Test for csv file with backspace as quote character and escape character in varchar field
	step "Import data file: test_backspace_char.csv -> test_backspace_char"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '|' \
			--file-table-map "test_backspace_char.csv:test_backspace_char" --escape-char="\b" --quote-char="\b"

	# Test2 for csv file with backspace as quote character and escape character in varchar field
	step "Import data file: test_backspace_char2.csv -> test_backspace_char2"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '|' \
			--file-table-map "test_backspace_char2.csv:test_backspace_char2" --escape-char="\b" --quote-char="\b"

	# Test for csv file with backspace as quote character and single quote as escape character
	step "Import data file: test_backspace_quote_single_quote_escape.csv -> test_backspace_quote_single_quote_escape"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '|' \
			--file-table-map "test_backspace_quote_single_quote_escape.csv:test_backspace_quote_single_quote_escape" --escape-char="'" --quote-char="\b"

	# Test for csv file with backspace as quote character and double quote as escape character
	step "Import data file: test_backspace_quote_double_quote_escape.csv -> test_backspace_quote_double_quote_escape"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '|' \
			--file-table-map "test_backspace_quote_double_quote_escape.csv:test_backspace_quote_double_quote_escape" --escape-char="\"" --quote-char="\b"

	# Test for csv file with backspace as escape character and double quote as quote character
	step "Import data file: test_backspace_escape_double_quote.csv -> test_backspace_escape_double_quote"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '|' \
			--file-table-map "test_backspace_escape_double_quote.csv:test_backspace_escape_double_quote" --escape-char="\b" --quote-char="\""

	# Test for csv file with delimiter as backspace char
	step "Import data file: test_delimiter_backspace.csv -> test_delimiter_backspace"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '\b' \
			--file-table-map "test_delimiter_backspace.csv:test_delimiter_backspace"  --has-header 1

	# Test for txt file with delimiter as backspace char
	step "Import data file: test_delimiter_backspace_text.txt -> test_delimiter_backspace_text"
	import_data_file --data-dir ${TEST_DIR} --format text --delimiter '\b' \
			--file-table-map "test_delimiter_backspace_text.txt:test_delimiter_backspace_text" 

	# Test import from s3 (text file)
	step "Import data file from S3 (text): text_test.text -> s3_text"
	import_data_file --data-dir ${S3_BUCKET} --format text --delimiter '\t' \
			--file-table-map "text_test.text:s3_text"

	# Test import from s3 (csv)
	step "Import data file from S3 (csv): csv_test.csv -> s3_csv"
	import_data_file --data-dir ${S3_BUCKET} --format csv --delimiter ',' \
			--file-table-map "csv_test.csv:s3_csv"
	
	# Test multi-table import from s3
	step "Import data file from multitable S3: t1.text-> s3_multitable_t1, t2.text -> s3_multitable_t2"
	import_data_file --data-dir ${S3_BUCKET} --format text --delimiter '\t' \
			--file-table-map "t1.text:s3_multitable_t1,t2.text:s3_multitable_t2"

	# Test csv with header import from s3
	step "Import data file from S3 (csv): csv_with_header_test.csv -> s3_csv_with_header"
	import_data_file --data-dir ${S3_BUCKET} --format csv --delimiter ',' \
			--file-table-map "csv_with_header_test.csv:s3_csv_with_header" --has-header y

	export MAX_BATCH_SIZE_BYTES=343245
	# Test larger import from s3
	step "Import large data file from S3: volume.text-> s3_volume"
	import_data_file --data-dir ${S3_BUCKET} --format text --delimiter '\t' \
			--file-table-map "volume_test:s3_volume"

	export MAX_BATCH_SIZE_BYTES=3450 #these are smaller files so its better to have smaller here 

	# Test import csv file with null_string as NIL
	step "Import data file: t1_null_string_csv.csv -> t1_null_string_csv"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '|' \
			--file-table-map "t1_null_string_csv.csv:t1_null_string_csv" --null-string "NIL"
		
	# Test import text file with null_string as NONE
	step "Import data file: t1_null_string_text.txt -> t1_null_string_text"
	import_data_file --data-dir ${TEST_DIR} --format text --delimiter '\t' \
			--file-table-map "t1_null_string_text.txt:t1_null_string_text" --null-string "NONE"

	# Test import csv file with null_string as \0
	step "Import data file: t1_null_string_csv2.csv -> t1_null_string_csv2"
	import_data_file --data-dir ${TEST_DIR} --format csv --delimiter '|' \
			--file-table-map "t1_null_string_csv2.csv:t1_null_string_csv2" --null-string "\0"
	
	# Test import text file with null_string as \0
	step "Import data file: t1_null_string_text2.txt -> t1_null_string_text2"
	import_data_file --data-dir ${TEST_DIR} --format text --delimiter '\t' \
			--file-table-map "t1_null_string_text2.txt:t1_null_string_text2" --null-string "\0"

	# Test importing multiple files to the same table.
	step "Import data file: foo[123].text --> foo"
	import_data_file --data-dir ${TEST_DIR} --format text --file-table-map "foo*.text:foo"

	step "Import multiple files to the same table. No regex in the file-table-map."
	import_data_file --data-dir ${TEST_DIR} --format text \
		--file-table-map "internal/foo3.text:foo2,foo1.text:foo2,foo2.text:foo2"

	step "Import multiple files to the same table. Same filename different path."
	import_data_file --data-dir ${TEST_DIR} --format text \
		--file-table-map "internal/foo1.text:foo3,foo1.text:foo3"
		
	# Test import from GCS buckets (text file)
	step "Import data file from GCS (text): text_test.text -> gcs_text"
	import_data_file --data-dir "gs://voyager-automation-data" --format text --delimiter '\t' \
			--file-table-map "text_test.text:gcs_text"

	# Test import from GCS buckets (csv file)
	step "Import data file from GCS (csv): csv_test.csv -> gcs_csv"
	import_data_file --data-dir "gs://voyager-automation-data" --format csv --delimiter ',' \
			--file-table-map "csv_test.csv:gcs_csv"

	# Test multi-table import from GCS buckets
	step "Import data file from multitable GCS: t1.text-> gcs_multitable_t1, t2.text -> gcs_multitable_t2"
	import_data_file --data-dir "gs://voyager-automation-data" --format text --delimiter '\t' \
			--file-table-map "t1.text:gcs_multitable_t1,t2.text:gcs_multitable_t2"

	# Test csv with header import from GCS buckets
	step "Import data file from GCS (csv): csv_with_header_test.csv -> gcs_csv_with_header"
	import_data_file --data-dir "gs://voyager-automation-data" --format csv --delimiter ',' \
			--file-table-map "csv_with_header_test.csv:gcs_csv_with_header" --has-header true

	export MAX_BATCH_SIZE_BYTES=343245

	# Test larger import from GCS buckets
	step "Import large data file from GCS: volume1.csv-> gcs_volume"
	import_data_file --data-dir "gs://voyager-automation-data" --format csv --delimiter '\t' \
			--file-table-map "volume1.csv:gcs_volume" --has-header yes --batch-size 10000 --null-string "\N"

	unset MAX_BATCH_SIZE_BYTES #unsetting it here as large import file test should run with defaults

	# Test import from GCS buckets with escape/quote characters
	step "Import data file from GCS (csv): t1_quote_escape_char1.csv -> gcs_quote_escape_char1"
	import_data_file --data-dir "gs://voyager-automation-data" --format csv --delimiter '|' \
			--file-table-map "t1_quote_escape_char1.csv:gcs_quote_escape_char1" --quote-char="'" --escape-char="'"
	
	
	if [ "${RUN_LARGE_IMPORT_DATA_FILE_TEST}" = true ] ; then

		step "Run large sized import data file test"
		import_data_file --data-dir "s3://yb-voyager-test-data" --delimiter "\t" --format "text" --file-table-map "accounts_350m_data.sql:accounts_large" --yes

	fi

	step "Run validations."
	"${TEST_DIR}/validate"
	
	step "End Migration: clearing metainfo about state of migration from everywhere."
	end_migration --yes

	step "Remove the credentials file for GCS default authentications"
	rm -rf ~/.config/gcloud/application_default_credentials.json
}

main
